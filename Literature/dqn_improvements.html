<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DQN Improvements and Variants</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: #333;
            overflow-x: hidden;
        }

        .presentation-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        .slide {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            border-radius: 15px;
            padding: 40px;
            margin-bottom: 30px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
            border: 1px solid rgba(255, 255, 255, 0.2);
            display: none;
            animation: slideIn 0.5s ease-out;
        }

        .slide.active {
            display: block;
        }

        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        h1 {
            color: #4a5568;
            text-align: center;
            margin-bottom: 30px;
            font-size: 2.5em;
            font-weight: 300;
            background: linear-gradient(135deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        h2 {
            color: #2d3748;
            margin-bottom: 25px;
            font-size: 2em;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
        }

        h3 {
            color: #4a5568;
            margin: 20px 0 15px 0;
            font-size: 1.3em;
        }

        .problem-solution {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 30px 0;
        }

        .problem-card {
            background: linear-gradient(135deg, #fff5f5, #fed7d7);
            border-radius: 10px;
            padding: 25px;
            border-left: 5px solid #f56565;
        }

        .solution-card {
            background: linear-gradient(135deg, #f0fff4, #c6f6d5);
            border-radius: 10px;
            padding: 25px;
            border-left: 5px solid #48bb78;
        }

        .formula {
            font-family: 'Courier New', monospace;
            font-size: 1.2em;
            background: #2d3748;
            color: #e2e8f0;
            padding: 15px;
            border-radius: 5px;
            margin: 15px 0;
            text-align: center;
        }

        .comparison {
            background: #f8f9fa;
            border-radius: 10px;
            padding: 25px;
            margin: 20px 0;
            border-left: 5px solid #764ba2;
        }

        .network-architecture {
            background: #f7fafc;
            border-radius: 10px;
            padding: 25px;
            margin: 20px 0;
            text-align: center;
        }

        .arch-flow {
            display: flex;
            justify-content: center;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 20px 0;
        }

        .arch-box {
            background: #667eea;
            color: white;
            padding: 12px 20px;
            border-radius: 8px;
            font-weight: bold;
            min-width: 120px;
            font-size: 0.9em;
        }

        .arch-box.state-value {
            background: #48bb78;
        }

        .arch-box.advantage {
            background: #ed8936;
        }

        .arch-box.combine {
            background: #9f7aea;
        }

        .arrow {
            font-size: 1.5em;
            color: #764ba2;
            font-weight: bold;
        }

        .highlight {
            background: linear-gradient(135deg, #fef5e7, #fed7aa);
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 4px solid #f6ad55;
        }

        .double-dqn-demo {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 25px 0;
        }

        .network-demo {
            background: #e6fffa;
            padding: 20px;
            border-radius: 10px;
            border: 2px solid #38b2ac;
            text-align: center;
        }

        .dueling-streams {
            display: flex;
            justify-content: space-around;
            align-items: flex-start;
            margin: 25px 0;
            gap: 20px;
        }

        .stream {
            flex: 1;
            background: #f0f4f8;
            padding: 20px;
            border-radius: 10px;
            border: 2px solid #4299e1;
            text-align: center;
        }

        .stream.value {
            border-color: #48bb78;
            background: #f0fff4;
        }

        .stream.advantage {
            border-color: #ed8936;
            background: #fffaf0;
        }

        .navigation {
            position: fixed;
            bottom: 30px;
            left: 50%;
            transform: translateX(-50%);
            display: flex;
            gap: 15px;
            background: rgba(255, 255, 255, 0.9);
            padding: 15px 25px;
            border-radius: 50px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);
            backdrop-filter: blur(10px);
        }

        .nav-btn {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 25px;
            cursor: pointer;
            font-size: 14px;
            transition: all 0.3s ease;
        }

        .nav-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.3);
        }

        .slide-number {
            position: absolute;
            top: 20px;
            right: 30px;
            background: rgba(255, 255, 255, 0.8);
            padding: 5px 15px;
            border-radius: 20px;
            font-size: 14px;
            color: #4a5568;
        }

        ul, ol {
            margin-left: 20px;
            line-height: 1.8;
        }

        li {
            margin-bottom: 8px;
        }

        .timeline {
            background: #f8f9fa;
            padding: 25px;
            border-radius: 10px;
            margin: 20px 0;
        }

        .timeline-item {
            display: flex;
            align-items: center;
            margin: 15px 0;
            padding: 15px;
            background: white;
            border-radius: 8px;
            border-left: 4px solid #667eea;
        }

        .year {
            font-weight: bold;
            color: #667eea;
            margin-right: 20px;
            min-width: 60px;
        }

        @media (max-width: 768px) {
            .slide {
                padding: 20px;
                margin: 10px;
            }
            
            .problem-solution, .double-dqn-demo {
                grid-template-columns: 1fr;
            }
            
            .dueling-streams {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
    <div class="presentation-container">
        <div class="slide-number" id="slideNumber">1 / 6</div>

        <!-- Slide 1: Title & Context -->
        <div class="slide active">
            <h1>DQN Improvements & Variants</h1>
            <div style="text-align: center; margin: 40px 0;">
                <div style="font-size: 1.5em; color: #4a5568; margin-bottom: 20px;">
                    Building on the Foundation
                </div>
                <div style="font-size: 1.1em; color: #718096; line-height: 1.6;">
                    While PPO, A3C, and DDPG explored different paradigms,<br>
                    the DQN family continued to evolve and improve
                </div>
            </div>

            <div class="timeline">
                <h3>Evolution Timeline</h3>
                <div class="timeline-item">
                    <span class="year">2015</span>
                    <div>
                        <strong>DQN:</strong> Original Deep Q-Network with experience replay and target networks
                    </div>
                </div>
                <div class="timeline-item">
                    <span class="year">2016</span>
                    <div>
                        <strong>Double DQN:</strong> Addresses overestimation bias using separate networks for action selection and evaluation
                    </div>
                </div>
                <div class="timeline-item">
                    <span class="year">2016</span>
                    <div>
                        <strong>Dueling DQN:</strong> New network architecture separating state values and action advantages
                    </div>
                </div>
                <div class="timeline-item">
                    <span class="year">2017</span>
                    <div>
                        <strong>Rainbow DQN:</strong> Combines multiple improvements for state-of-the-art performance
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 2: DQN Problems -->
        <div class="slide">
            <h2>Problems with Original DQN</h2>
            
            <div style="background: #fff5f5; padding: 25px; border-radius: 10px; border-left: 5px solid #f56565; margin: 30px 0;">
                <h3>Key Issues Identified:</h3>
                
                <div style="margin: 20px 0;">
                    <h4>üéØ Overestimation Bias</h4>
                    <p>DQN systematically overestimates Q-values due to the max operator in the Bellman equation</p>
                    <div class="formula" style="font-size: 1em;">
                        Target: y = r + Œ≥ max_a' Q(s',a'; Œ∏‚Åª)
                    </div>
                    <p><em>Problem: The same network both selects and evaluates actions</em></p>
                </div>

                <div style="margin: 20px 0;">
                    <h4>üß† Inefficient Learning</h4>
                    <p>The network learns Q(s,a) directly, but doesn't explicitly separate:</p>
                    <ul>
                        <li><strong>State value:</strong> "How good is this state overall?"</li>
                        <li><strong>Action advantage:</strong> "How much better is this action compared to others?"</li>
                    </ul>
                </div>

                <div style="margin: 20px 0;">
                    <h4>üìä Sample Inefficiency</h4>
                    <p>Network updates all Q-values equally, even when only action advantages change</p>
                </div>
            </div>

            <div class="highlight">
                <strong>Impact:</strong> These issues led to slower convergence and suboptimal performance, especially in complex environments
            </div>
        </div>

        <!-- Slide 3: Double DQN -->
        <div class="slide">
            <h2>Double DQN: Addressing Overestimation</h2>
            
            <div class="problem-solution">
                <div class="problem-card">
                    <h3>üî¥ Original DQN Problem</h3>
                    <p><strong>Single Network Does Both:</strong></p>
                    <ol>
                        <li>Select best action: a* = argmax Q(s',a')</li>
                        <li>Evaluate that action: Q(s',a*)</li>
                    </ol>
                    <div class="formula" style="font-size: 0.9em;">
                        y = r + Œ≥ max_a' Q(s',a'; Œ∏‚Åª)
                    </div>
                    <p><em>This leads to positive bias!</em></p>
                </div>
                
                <div class="solution-card">
                    <h3>üü¢ Double DQN Solution</h3>
                    <p><strong>Separate Responsibilities:</strong></p>
                    <ol>
                        <li>Online network selects: a* = argmax Q(s',a'; Œ∏)</li>
                        <li>Target network evaluates: Q(s',a*; Œ∏‚Åª)</li>
                    </ol>
                    <div class="formula" style="font-size: 0.9em;">
                        y = r + Œ≥ Q(s', a*; Œ∏‚Åª)
                    </div>
                    <p><em>Reduces overestimation bias!</em></p>
                </div>
            </div>

            <div class="double-dqn-demo">
                <div class="network-demo">
                    <h4>üéØ Online Network (Œ∏)</h4>
                    <p><strong>Role:</strong> Action Selection</p>
                    <div class="arch-flow" style="font-size: 0.8em;">
                        <div class="arch-box">State s'</div>
                        <span class="arrow">‚Üí</span>
                        <div class="arch-box">Q-values</div>
                        <span class="arrow">‚Üí</span>
                        <div class="arch-box">argmax ‚Üí a*</div>
                    </div>
                    <p><em>Finds the best action</em></p>
                </div>
                
                <div class="network-demo">
                    <h4>üéñÔ∏è Target Network (Œ∏‚Åª)</h4>
                    <p><strong>Role:</strong> Value Evaluation</p>
                    <div class="arch-flow" style="font-size: 0.8em;">
                        <div class="arch-box">State s'</div>
                        <span class="arrow">‚Üí</span>
                        <div class="arch-box">Q-values</div>
                        <span class="arrow">‚Üí</span>
                        <div class="arch-box">Q(s', a*)</div>
                    </div>
                    <p><em>Evaluates the selected action</em></p>
                </div>
            </div>

            <div class="comparison">
                <h3>Mathematical Comparison</h3>
                <p><strong>Original DQN:</strong></p>
                <div class="formula">y = r + Œ≥ max_a' Q(s',a'; Œ∏‚Åª)</div>
                
                <p><strong>Double DQN:</strong></p>
                <div class="formula">
                    a* = argmax_a' Q(s',a'; Œ∏)<br>
                    y = r + Œ≥ Q(s', a*; Œ∏‚Åª)
                </div>
                
                <p><strong>Key Insight:</strong> By decoupling action selection from value evaluation, we reduce the positive bias that leads to overestimation.</p>
            </div>
        </div>

        <!-- Slide 4: Dueling DQN Architecture -->
        <div class="slide">
            <h2>Dueling DQN: Architectural Innovation</h2>
            
            <div class="highlight">
                <strong>Core Idea:</strong> Instead of directly learning Q(s,a), learn V(s) and A(s,a) separately, then combine them
            </div>

            <div class="network-architecture">
                <h3>Traditional DQN vs Dueling DQN</h3>
                
                <div style="margin: 30px 0;">
                    <h4>üî∑ Traditional DQN Architecture</h4>
                    <div class="arch-flow">
                        <div class="arch-box">State s</div>
                        <span class="arrow">‚Üí</span>
                        <div class="arch-box">Conv Layers</div>
                        <span class="arrow">‚Üí</span>
                        <div class="arch-box">FC Layers</div>
                        <span class="arrow">‚Üí</span>
                        <div class="arch-box">Q(s,a) for all a</div>
                    </div>
                </div>

                <div style="margin: 30px 0;">
                    <h4>üî∂ Dueling DQN Architecture</h4>
                    <div class="arch-flow">
                        <div class="arch-box">State s</div>
                        <span class="arrow">‚Üí</span>
                        <div class="arch-box">Conv Layers</div>
                        <span class="arrow">‚Üí</span>
                        <div class="arch-box">Common FC</div>
                        <span class="arrow">‚Üí</span>
                        <div style="display: flex; flex-direction: column; gap: 10px;">
                            <div class="arch-box state-value">V(s) Stream</div>
                            <div class="arch-box advantage">A(s,a) Stream</div>
                        </div>
                        <span class="arrow">‚Üí</span>
                        <div class="arch-box combine">Combine ‚Üí Q(s,a)</div>
                    </div>
                </div>
            </div>

            <div class="dueling-streams">
                <div class="stream value">
                    <h4>üü¢ State-Value Function V(s)</h4>
                    <p><strong>Represents:</strong> How good the current state is overall, independent of specific actions</p>
                    <p><strong>Example:</strong> "The system is lightly loaded, so this state is already good"</p>
                    <p><strong>Output:</strong> Single scalar value</p>
                </div>
                
                <div class="stream advantage">
                    <h4>üü† Advantage Function A(s,a)</h4>
                    <p><strong>Represents:</strong> The relative importance of each action compared to others in the same state</p>
                    <p><strong>Example:</strong> "Assigning task to edge node is slightly better than cloud in this state"</p>
                    <p><strong>Output:</strong> Vector of advantage values</p>
                </div>
            </div>
        </div>

        <!-- Slide 5: Dueling DQN Mathematics -->
        <div class="slide">
            <h2>Dueling DQN: Mathematical Foundation</h2>
            
            <div class="comparison">
                <h3>Decomposition Formula</h3>
                <p>The key insight is that any Q-function can be decomposed as:</p>
                <div class="formula">
                    Q(s,a) = V(s) + A(s,a)
                </div>
                <p>where V(s) is the state value and A(s,a) is the action advantage</p>
            </div>

            <div class="comparison">
                <h3>Identifiability Problem</h3>
                <p>However, this decomposition is not unique! We need a constraint:</p>
                <div class="formula">
                    Q(s,a) = V(s) + A(s,a) - max_a' A(s,a')
                </div>
                <p><strong>Or alternatively (more stable in practice):</strong></p>
                <div class="formula">
                    Q(s,a) = V(s) + A(s,a) - (1/|A|) Œ£_a' A(s,a')
                </div>
                <p>This ensures that the advantage of the best action is zero on average</p>
            </div>

            <div style="background: #f0f4f8; padding: 25px; border-radius: 10px; margin: 25px 0;">
                <h3>Why This Works Better</h3>
                <ol>
                    <li><strong>Faster Learning:</strong> V(s) can be updated from any experience, not just when that action is taken</li>
                    <li><strong>Better Generalization:</strong> The network learns what makes states good vs. what makes actions good</li>
                    <li><strong>Robustness:</strong> Less sensitive to irrelevant actions in states where only a few actions matter</li>
                </ol>
            </div>

            <div class="highlight">
                <strong>Intuition:</strong> In many states, it doesn't matter which action you take (they're all good or all bad). Dueling DQN can represent this efficiently by learning a high/low V(s) with small advantage differences.
            </div>

            <div style="background: #e6fffa; padding: 20px; border-radius: 10px; border: 2px solid #38b2ac; margin: 20px 0;">
                <h4>üéØ Example: Traffic Light Control</h4>
                <ul>
                    <li><strong>V(s):</strong> "Rush hour traffic = bad state" vs "Light traffic = good state"</li>
                    <li><strong>A(s,a):</strong> "In rush hour, extending green for main road has +0.1 advantage over switching"</li>
                    <li><strong>Result:</strong> Network learns traffic patterns (V) separately from timing decisions (A)</li>
                </ul>
            </div>
        </div>

        <!-- Slide 6: Summary & Impact -->
        <div class="slide">
            <h2>Summary & Impact</h2>
            
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin: 30px 0;">
                <div style="background: linear-gradient(135deg, #e6fffa, #f0fff4); padding: 25px; border-radius: 10px; border: 2px solid #38b2ac;">
                    <h3>üéØ Double DQN Benefits</h3>
                    <ul>
                        <li><strong>Reduced Bias:</strong> Eliminates systematic overestimation</li>
                        <li><strong>Better Performance:</strong> More accurate Q-value estimates</li>
                        <li><strong>Stable Learning:</strong> Less volatile training curves</li>
                        <li><strong>Easy Implementation:</strong> Minimal code changes needed</li>
                    </ul>
                </div>
                
                <div style="background: linear-gradient(135deg, #fff5f5, #fed7d7); padding: 25px; border-radius: 10px; border: 2px solid #f56565;">
                    <h3>üèóÔ∏è Dueling DQN Benefits</h3>
                    <ul>
                        <li><strong>Faster Learning:</strong> More efficient use of experience</li>
                        <li><strong>Better Generalization:</strong> Separates state quality from action choice</li>
                        <li><strong>Robustness:</strong> Handles irrelevant actions better</li>
                        <li><strong>Architectural Innovation:</strong> Can be combined with other methods</li>
                    </ul>
                </div>
            </div>

            <div class="comparison">
                <h3>Performance Comparison</h3>
                <p><strong>Atari 2600 Results (Human Normalized Score):</strong></p>
                <ul>
                    <li>DQN: ~100% (baseline)</li>
                    <li>Double DQN: ~130% (+30% improvement)</li>
                    <li>Dueling DQN: ~150% (+50% improvement)</li>
                    <li>Double Dueling DQN: ~180% (+80% improvement)</li>
                </ul>
            </div>

            <div class="highlight">
                <strong>Key Insight:</strong> These improvements are orthogonal and can be combined! The Rainbow DQN (2017) combines:
                <br>‚Ä¢ Double DQN ‚Ä¢ Dueling Networks ‚Ä¢ Prioritized Replay ‚Ä¢ Distributional RL ‚Ä¢ Multi-step Learning ‚Ä¢ Noisy Networks
            </div>

            <div style="text-align: center; margin: 40px 0;">
                <div style="background: linear-gradient(135deg, #667eea, #764ba2); color: white; padding: 30px; border-radius: 15px;">
                    <h3 style="color: white; margin-bottom: 15px;">The Evolution Continues...</h3>
                    <p style="font-size: 1.1em; line-height: 1.6;">
                        While PPO, A3C, and DDPG explored policy gradients and actor-critic methods,<br>
                        DQN variants showed that value-based methods still had tremendous potential.<br>
                        Each approach has its strengths for different problem domains.
                    </p>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation">
        <button class="nav-btn" onclick="previousSlide()">‚Üê Previous</button>
        <button class="nav-btn" onclick="nextSlide()">Next ‚Üí</button>
    </div>

    <script>
        let currentSlide = 0;
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;

        function showSlide(n) {
            slides.forEach(slide => slide.classList.remove('active'));
            slides[n].classList.add('active');
            document.getElementById('slideNumber').textContent = `${n + 1} / ${totalSlides}`;
        }

        function nextSlide() {
            currentSlide = (currentSlide + 1) % totalSlides;
            showSlide(currentSlide);
        }

        function previousSlide() {
            currentSlide = (currentSlide - 1 + totalSlides) % totalSlides;
            showSlide(currentSlide);
        }

        // Keyboard navigation
        document.addEventListener('keydown', function(event) {
            if (event.key === 'ArrowRight' || event.key === ' ') {
                nextSlide();
            } else if (event.key === 'ArrowLeft') {
                previousSlide();
            }
        });

        // Initialize
        showSlide(0);
    </script>
</body>
</html>