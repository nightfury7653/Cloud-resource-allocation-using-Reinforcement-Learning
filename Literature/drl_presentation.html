<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Reinforcement Learning</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: #333;
            overflow-x: hidden;
        }

        .presentation-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        .slide {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            border-radius: 15px;
            padding: 40px;
            margin-bottom: 30px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
            border: 1px solid rgba(255, 255, 255, 0.2);
            display: none;
            animation: slideIn 0.5s ease-out;
        }

        .slide.active {
            display: block;
        }

        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        h1 {
            color: #4a5568;
            text-align: center;
            margin-bottom: 30px;
            font-size: 2.5em;
            font-weight: 300;
            background: linear-gradient(135deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        h2 {
            color: #2d3748;
            margin-bottom: 25px;
            font-size: 2em;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
        }

        h3 {
            color: #4a5568;
            margin: 20px 0 15px 0;
            font-size: 1.3em;
        }

        .learning-types {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .learning-card {
            background: linear-gradient(135deg, #f7fafc, #edf2f7);
            border-radius: 10px;
            padding: 25px;
            border-left: 5px solid #667eea;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .learning-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 25px rgba(0, 0, 0, 0.15);
        }

        .rl-components {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .component-card {
            background: linear-gradient(135deg, #e6fffa, #f0fff4);
            border-radius: 10px;
            padding: 20px;
            text-align: center;
            border: 2px solid #38b2ac;
            transition: all 0.3s ease;
        }

        .component-card:hover {
            transform: scale(1.05);
            border-color: #319795;
        }

        .math-section {
            background: #f8f9fa;
            border-radius: 10px;
            padding: 25px;
            margin: 20px 0;
            border-left: 5px solid #764ba2;
        }

        .formula {
            font-family: 'Courier New', monospace;
            font-size: 1.2em;
            background: #2d3748;
            color: #e2e8f0;
            padding: 15px;
            border-radius: 5px;
            margin: 15px 0;
            text-align: center;
        }

        .approach-comparison {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 30px 0;
        }

        .approach-card {
            background: linear-gradient(135deg, #fff5f5, #fed7d7);
            border-radius: 10px;
            padding: 25px;
            border: 2px solid #f56565;
        }

        .approach-card:nth-child(2) {
            background: linear-gradient(135deg, #f0fff4, #c6f6d5);
            border-color: #48bb78;
        }

        .navigation {
            position: fixed;
            bottom: 30px;
            left: 50%;
            transform: translateX(-50%);
            display: flex;
            gap: 15px;
            background: rgba(255, 255, 255, 0.9);
            padding: 15px 25px;
            border-radius: 50px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);
            backdrop-filter: blur(10px);
        }

        .nav-btn {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 25px;
            cursor: pointer;
            font-size: 14px;
            transition: all 0.3s ease;
        }

        .nav-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.3);
        }

        .slide-number {
            position: absolute;
            top: 20px;
            right: 30px;
            background: rgba(255, 255, 255, 0.8);
            padding: 5px 15px;
            border-radius: 20px;
            font-size: 14px;
            color: #4a5568;
        }

        ul {
            margin-left: 20px;
            line-height: 1.8;
        }

        li {
            margin-bottom: 10px;
        }

        .highlight {
            background: linear-gradient(135deg, #fef5e7, #fed7aa);
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #f6ad55;
        }

        .dqn-architecture {
            background: #f7fafc;
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            text-align: center;
        }

        .architecture-flow {
            display: flex;
            justify-content: space-around;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 20px 0;
        }

        .arch-box {
            background: #667eea;
            color: white;
            padding: 15px 20px;
            border-radius: 8px;
            font-weight: bold;
            min-width: 120px;
        }

        .arrow {
            font-size: 2em;
            color: #764ba2;
        }

        @media (max-width: 768px) {
            .slide {
                padding: 20px;
                margin: 10px;
            }
            
            .approach-comparison {
                grid-template-columns: 1fr;
            }
            
            .navigation {
                bottom: 20px;
                padding: 10px 15px;
            }
            
            .nav-btn {
                padding: 8px 15px;
                font-size: 12px;
            }
        }
    </style>
</head>
<body>
    <div class="presentation-container">
        <div class="slide-number" id="slideNumber">1 / 8</div>

        <!-- Slide 1: Title -->
        <div class="slide active">
            <h1>Deep Reinforcement Learning</h1>
            <div style="text-align: center; margin: 50px 0;">
                <div style="font-size: 1.5em; color: #4a5568; margin-bottom: 20px;">
                    From Basics to Deep Q-Networks
                </div>
                <div style="font-size: 1.1em; color: #718096;">
                    A comprehensive guide to understanding DRL algorithms
                </div>
            </div>
            <div style="text-align: center; margin-top: 60px;">
                <div style="background: rgba(102, 126, 234, 0.1); padding: 30px; border-radius: 15px; display: inline-block;">
                    <div style="font-size: 1.2em; color: #2d3748; margin-bottom: 10px;">Presentation Overview</div>
                    <div style="color: #4a5568; line-height: 1.6;">
                        Machine Learning Types ‚Üí RL Fundamentals ‚Üí Mathematical Framework ‚Üí DQN Architecture
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 2: Learning Types -->
        <div class="slide">
            <h2>Types of Machine Learning</h2>
            <div class="learning-types">
                <div class="learning-card">
                    <h3 style="color: #2b6cb0;">üéØ Supervised Learning</h3>
                    <p><strong>Data:</strong> Input-output pairs (X, Y)</p>
                    <p><strong>Goal:</strong> Learn mapping f: X ‚Üí Y</p>
                    <p><strong>Examples:</strong> Classification, Regression</p>
                    <p><strong>Feedback:</strong> Immediate correct answers</p>
                </div>
                <div class="learning-card">
                    <h3 style="color: #2d3748;">üîç Unsupervised Learning</h3>
                    <p><strong>Data:</strong> Only inputs (X)</p>
                    <p><strong>Goal:</strong> Discover hidden patterns</p>
                    <p><strong>Examples:</strong> Clustering, Dimensionality Reduction</p>
                    <p><strong>Feedback:</strong> No explicit feedback</p>
                </div>
                <div class="learning-card">
                    <h3 style="color: #38a169;">üéÆ Reinforcement Learning</h3>
                    <p><strong>Data:</strong> States, Actions, Rewards</p>
                    <p><strong>Goal:</strong> Maximize cumulative reward</p>
                    <p><strong>Examples:</strong> Game playing, Robotics</p>
                    <p><strong>Feedback:</strong> Delayed rewards/punishments</p>
                </div>
            </div>
            <div class="highlight">
                <strong>Key Difference:</strong> RL learns through trial-and-error interaction with an environment, receiving sparse and delayed feedback.
            </div>
        </div>

        <!-- Slide 3: RL Fundamentals -->
        <div class="slide">
            <h2>Reinforcement Learning Fundamentals</h2>
            <div class="rl-components">
                <div class="component-card">
                    <h3>ü§ñ Agent</h3>
                    <p>The learner/decision maker that takes actions</p>
                </div>
                <div class="component-card">
                    <h3>üåç Environment</h3>
                    <p>The world in which the agent operates</p>
                </div>
                <div class="component-card">
                    <h3>üìç State (S)</h3>
                    <p>Current situation/configuration of the environment</p>
                </div>
                <div class="component-card">
                    <h3>‚ö° Action (A)</h3>
                    <p>Choices available to the agent in each state</p>
                </div>
                <div class="component-card">
                    <h3>üéÅ Reward (R)</h3>
                    <p>Immediate feedback after taking an action</p>
                </div>
                <div class="component-card">
                    <h3>üéØ Goal</h3>
                    <p>Maximize cumulative reward over time</p>
                </div>
            </div>
            <div style="text-align: center; margin: 30px 0;">
                <div style="background: #e6fffa; padding: 20px; border-radius: 10px; border: 2px solid #38b2ac;">
                    <strong>RL Loop:</strong> State ‚Üí Action ‚Üí Reward ‚Üí New State ‚Üí Action ‚Üí ...
                </div>
            </div>
        </div>

        <!-- Slide 4: Mathematical Framework -->
        <div class="slide">
            <h2>Mathematical Framework</h2>
            
            <div class="math-section">
                <h3>Reward Function</h3>
                <p><strong>Immediate Reward:</strong> R(s, a) - reward for taking action 'a' in state 's'</p>
                <div class="formula">R_t = R(S_t, A_t)</div>
                
                <p><strong>Discounted Cumulative Reward (Return):</strong></p>
                <div class="formula">G_t = R_{t+1} + Œ≥R_{t+2} + Œ≥¬≤R_{t+3} + ... = Œ£(Œ≥·µè R_{t+k+1})</div>
                <p>where Œ≥ ‚àà [0,1] is the discount factor</p>
            </div>

            <div class="math-section">
                <h3>Key Functions</h3>
                <p><strong>Q-Function (Action-Value):</strong> Expected return starting from state s, taking action a</p>
                <div class="formula">Q^œÄ(s,a) = E[G_t | S_t = s, A_t = a, œÄ]</div>
                
                <p><strong>Policy Function:</strong> Probability of taking action a in state s</p>
                <div class="formula">œÄ(a|s) = P(A_t = a | S_t = s)</div>
            </div>

            <div class="highlight">
                <strong>Goal:</strong> Find optimal policy œÄ* that maximizes expected return from any state
            </div>
        </div>

        <!-- Slide 5: Learning Approaches -->
        <div class="slide">
            <h2>Two Main Learning Approaches</h2>
            <div class="approach-comparison">
                <div class="approach-card">
                    <h3>üìä Value Learning</h3>
                    <p><strong>Strategy:</strong> Learn the value of states/actions</p>
                    <p><strong>Key Idea:</strong> If we know Q(s,a) for all actions, we can choose the best one</p>
                    <p><strong>Policy Derivation:</strong></p>
                    <div class="formula" style="font-size: 1em;">œÄ(s) = argmax_a Q(s,a)</div>
                    <p><strong>Methods:</strong> Q-Learning, SARSA, DQN</p>
                    <p><strong>Pros:</strong> Sample efficient, stable</p>
                    <p><strong>Cons:</strong> Limited to discrete actions</p>
                </div>
                <div class="approach-card">
                    <h3>üéØ Policy Learning</h3>
                    <p><strong>Strategy:</strong> Directly learn the optimal policy</p>
                    <p><strong>Key Idea:</strong> Optimize policy parameters to maximize expected return</p>
                    <p><strong>Objective:</strong></p>
                    <div class="formula" style="font-size: 1em;">J(Œ∏) = E[G_t | œÄ_Œ∏]</div>
                    <p><strong>Methods:</strong> REINFORCE, A3C, PPO</p>
                    <p><strong>Pros:</strong> Works with continuous actions</p>
                    <p><strong>Cons:</strong> High variance, less stable</p>
                </div>
            </div>
            <div style="text-align: center; margin-top: 20px;">
                <div style="background: #f0f4f8; padding: 15px; border-radius: 8px;">
                    <strong>Modern Approach:</strong> Actor-Critic methods combine both approaches
                </div>
            </div>
        </div>

        <!-- Slide 6: Deep Q-Networks Introduction -->
        <div class="slide">
            <h2>Deep Q-Networks (DQN)</h2>
            
            <div class="highlight">
                <strong>Problem with Traditional Q-Learning:</strong> Cannot handle large state spaces (e.g., images with 256¬≥ possible pixel values)
            </div>

            <div class="dqn-architecture">
                <h3>DQN Solution: Function Approximation</h3>
                <div class="architecture-flow">
                    <div class="arch-box">State (s)</div>
                    <span class="arrow">‚Üí</span>
                    <div class="arch-box">Neural Network</div>
                    <span class="arrow">‚Üí</span>
                    <div class="arch-box">Q-values for all actions</div>
                </div>
                <p>Q(s,a) ‚âà Q(s,a; Œ∏) where Œ∏ are the neural network parameters</p>
            </div>

            <div class="math-section">
                <h3>Key Innovations</h3>
                <ul>
                    <li><strong>Experience Replay:</strong> Store and randomly sample past experiences</li>
                    <li><strong>Target Network:</strong> Separate network for stable target computation</li>
                    <li><strong>CNN Architecture:</strong> Process raw pixel inputs</li>
                </ul>
            </div>

            <div style="background: #e6fffa; padding: 20px; border-radius: 10px; margin-top: 20px;">
                <strong>Achievement:</strong> First algorithm to learn control policies directly from high-dimensional sensory input using RL
            </div>
        </div>

        <!-- Slide 7: DQN Loss Function Derivation -->
        <div class="slide">
            <h2>DQN Loss Function Derivation</h2>
            
            <div class="math-section">
                <h3>Step 1: Bellman Equation</h3>
                <p>The optimal Q-function satisfies:</p>
                <div class="formula">Q*(s,a) = E[r + Œ≥ max_a' Q*(s',a')]</div>
            </div>

            <div class="math-section">
                <h3>Step 2: Target Value</h3>
                <p>For a transition (s, a, r, s'), the target is:</p>
                <div class="formula">y = r + Œ≥ max_a' Q(s',a'; Œ∏‚Åª)</div>
                <p>where Œ∏‚Åª are the parameters of the target network</p>
            </div>

            <div class="math-section">
                <h3>Step 3: Loss Function</h3>
                <p>We want our network prediction Q(s,a; Œ∏) to match the target y:</p>
                <div class="formula">L(Œ∏) = E[(y - Q(s,a; Œ∏))¬≤]</div>
                <p>Expanding:</p>
                <div class="formula">L(Œ∏) = E[(r + Œ≥ max_a' Q(s',a'; Œ∏‚Åª) - Q(s,a; Œ∏))¬≤]</div>
            </div>

            <div class="math-section">
                <h3>Step 4: Gradient</h3>
                <p>To minimize loss using gradient descent:</p>
                <div class="formula">‚àá_Œ∏ L(Œ∏) = E[(y - Q(s,a; Œ∏)) ‚àá_Œ∏ Q(s,a; Œ∏)]</div>
            </div>

            <div class="highlight">
                <strong>Key Insight:</strong> The target network Œ∏‚Åª is kept fixed for several steps to ensure training stability
            </div>
        </div>

        <!-- Slide 8: DQN Algorithm Summary -->
        <div class="slide">
            <h2>DQN Algorithm Summary</h2>
            
            <div style="background: #f8f9fa; padding: 25px; border-radius: 10px; border-left: 5px solid #667eea;">
                <h3>Algorithm Steps:</h3>
                <ol style="line-height: 2; margin-left: 20px;">
                    <li><strong>Initialize</strong> main network Q(s,a; Œ∏) and target network Q(s,a; Œ∏‚Åª)</li>
                    <li><strong>Initialize</strong> replay buffer D</li>
                    <li><strong>For each episode:</strong>
                        <ul style="margin: 10px 0;">
                            <li>Select action using Œµ-greedy: a = argmax_a Q(s,a; Œ∏) with probability 1-Œµ</li>
                            <li>Execute action, observe reward r and next state s'</li>
                            <li>Store transition (s,a,r,s') in replay buffer D</li>
                            <li>Sample random minibatch from D</li>
                            <li>Compute targets and update main network</li>
                            <li>Periodically update target network: Œ∏‚Åª ‚Üê Œ∏</li>
                        </ul>
                    </li>
                </ol>
            </div>

            <div class="approach-comparison" style="margin-top: 30px;">
                <div style="background: linear-gradient(135deg, #e6fffa, #f0fff4); padding: 20px; border-radius: 10px; border: 2px solid #38b2ac;">
                    <h3>Advantages</h3>
                    <ul>
                        <li>Handles high-dimensional states</li>
                        <li>Model-free learning</li>
                        <li>End-to-end training</li>
                        <li>Proven success in Atari games</li>
                    </ul>
                </div>
                <div style="background: linear-gradient(135deg, #fff5f5, #fed7d7); padding: 20px; border-radius: 10px; border: 2px solid #f56565;">
                    <h3>Limitations</h3>
                    <ul>
                        <li>Sample inefficiency</li>
                        <li>Overestimation bias</li>
                        <li>Limited to discrete actions</li>
                        <li>Sensitive to hyperparameters</li>
                    </ul>
                </div>
            </div>

            <div class="highlight" style="margin-top: 20px;">
                <strong>Impact:</strong> DQN opened the door to deep reinforcement learning and sparked numerous improvements like Double DQN, Dueling DQN, and Rainbow DQN
            </div>
        </div>
    </div>

    <div class="navigation">
        <button class="nav-btn" onclick="previousSlide()">‚Üê Previous</button>
        <button class="nav-btn" onclick="nextSlide()">Next ‚Üí</button>
    </div>

    <script>
        let currentSlide = 0;
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;

        function showSlide(n) {
            slides.forEach(slide => slide.classList.remove('active'));
            slides[n].classList.add('active');
            document.getElementById('slideNumber').textContent = `${n + 1} / ${totalSlides}`;
        }

        function nextSlide() {
            currentSlide = (currentSlide + 1) % totalSlides;
            showSlide(currentSlide);
        }

        function previousSlide() {
            currentSlide = (currentSlide - 1 + totalSlides) % totalSlides;
            showSlide(currentSlide);
        }

        // Keyboard navigation
        document.addEventListener('keydown', function(event) {
            if (event.key === 'ArrowRight' || event.key === ' ') {
                nextSlide();
            } else if (event.key === 'ArrowLeft') {
                previousSlide();
            }
        });

        // Initialize
        showSlide(0);
    </script>
</body>
</html>